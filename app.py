# --------------------------
# Streamlit-app f√∂r Bibel-Chatbot
# --------------------------

import os
import openai
import streamlit as st
from dotenv import load_dotenv, find_dotenv
import zipfile
import gdown

# --------------------------
# KONFIGURATION & S√ÑKERHET
# --------------------------

# Ladda .env-fil och kontrollera API-nyckeln
env_path = find_dotenv()
if not env_path:
    raise FileNotFoundError(
        "‚ö†Ô∏è .env-fil inte hittad. Se till att den ligger i projektets rotkatalog."
    )
load_dotenv(env_path)

# H√§mta API-nyckel
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("üîë API-nyckel saknas i .env-filen. Kontrollera att OPENAI_API_KEY √§r definierad.")
# S√§tt API-nyckeln f√∂r `openai`-paketet
oai = openai
oai.api_key = api_key

# --------------------------
# IMPORTERA BIBLIOTEK OCH MODULER
# --------------------------

from langchain.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# --------------------------
# SIDKONFIGURATION OCH DESIGN
# --------------------------

st.set_page_config(
    page_title="üìñ Bibeln RAG-Chatbot", 
    layout="wide"                       
)

# --------------------------
# SKAPA EGNA PROMPT-TEMPLATE
# --------------------------

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Du √§r en v√§nlig och hj√§lpsam bibelguide som svarar p√• fr√•gor genom att anv√§nda givna bibelavsnitt.
Om du inte hittar relevant information ska du be anv√§ndaren om att skriva om fr√•gan.

Kontekst:
{context}

Fr√•ga:
{question}

Svar:
"""
)

# --------------------------
# FUNKTION F√ñR ATT LADDA FAISS-INDEX
# --------------------------

@st.cache_resource
def load_retriever():
    """
    Ladda FAISS-index fr√•n Google Drive om det inte finns lokalt,
    och skapa en retriever-objekt.
    """
    index_path = "data/faiss_index"
    zip_path = "data.zip"
    
    if not os.path.exists(index_path):
        with st.spinner("F√∂rbereder kunskapsbas..."):
            # Skapa mapp om den inte finns
            os.makedirs("data", exist_ok=True)
            
            # Ladda ner fr√•n Google Drive om zip-fil saknas
            if not os.path.exists(zip_path):
                try:
                    gdown.download(
                        "https://drive.google.com/file/d/1bdspw4vRQ6Ui0oaTE91B5WyVEz0vD8cA/view?usp=drive_link",
                        zip_path,
                        quiet=False
                    )
                except Exception as e:
                    st.error(f"Kunde inte ladda ner fil: {str(e)}")
                    st.stop()
            
            # Verifiera och packa upp zip-filen
            try:
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall("data/")
            except zipfile.BadZipFile:
                st.error("Filen √§r inte en giltig ZIP. Kontrollera Google Drive-l√§nken.")
                st.stop()
            except Exception as e:
                st.error(f"Fel vid uppackning: {str(e)}")
                st.stop()
    
    # Ladda FAISS-index
    try:
        embeddings = OpenAIEmbeddings()
        store = FAISS.load_local(
            index_path,
            embeddings,
            allow_dangerous_deserialization=True
        )
        return store.as_retriever(search_kwargs={"k": 3})
    except Exception as e:
        st.error(f"Fel vid laddning av FAISS-index: {str(e)}")
        st.stop()

# --------------------------
# L√ÑS IN FAISS-INDEXET
# --------------------------

with st.spinner("Laddar kunskapsbas..."):
    retriever = load_retriever()

# --------------------------
# INITIERA LLM OCH QA-KEDJA
# --------------------------

llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",  
    temperature=0                
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=False,
    chain_type_kwargs={"prompt": prompt}
)

# --------------------------
# KONVERSATIONS-SESSIONSTATE
# --------------------------

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Hej! Jag √§r din Bibel-Chatbot. Fr√•ga g√§rna om n√•got bibelst√§lle eller tema, s√• hj√§lper jag dig!"}
    ]

# --------------------------
# RENDERA MESSAGES SOM CHATTBUBBLOR
# --------------------------

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# --------------------------
# INPUTF√ÑLT F√ñR ANV√ÑNDARENS FR√ÖGA
# --------------------------

if user_input := st.chat_input("Skriv din fr√•ga h√§r..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    try:
        answer = qa_chain.run(user_input)
        if not answer.strip():
            answer = "Jag f√∂rst√•r inte riktigt. Kan du formulera fr√•gan p√• ett annat s√§tt?"
    except Exception as e:
        answer = "‚ùå Ett fel uppstod vid generering av svaret. F√∂rs√∂k igen senare."
        st.error(f"Detaljerat fel: {e}")

    st.session_state.messages.append({"role": "assistant", "content": answer})
    st.chat_message("assistant").write(answer)

# --------------------------
# FOOTER
# --------------------------

st.markdown("---")
st.caption("""
üìñ *Svenska Bibel-Chatbot v1.0* | 
Datak√§lla: Svenska Bibels√§llskapet | 
Byggd med Python & LangChain
""")