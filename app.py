# --------------------------
# Streamlit-app f√∂r Bibel-Chatbot
# --------------------------

import os
import openai
import streamlit as st
from dotenv import load_dotenv, find_dotenv

# --------------------------
# KONFIGURATION & S√ÑKERHET
# --------------------------

# Ladda .env-fil och kontrollera API-nyckeln
env_path = find_dotenv()
if not env_path:
    raise FileNotFoundError(
        "‚ö†Ô∏è .env-fil inte hittad. Se till att den ligger i projektets rotkatalog."
    )
load_dotenv(env_path)

# H√§mta API-nyckel
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("üîë API-nyckel saknas i .env-filen. Kontrollera att OPENAI_API_KEY √§r definierad.")
# S√§tt API-nyckeln f√∂r `openai`-paketet
oai = openai
oai.api_key = api_key

# --------------------------
# IMPORTERA BIBLIOTEK OCH MODULER
# --------------------------

# LangChain-klasser f√∂r embeddings, vektorlagring, chat-modell, prompt och kedja.
from langchain.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# --------------------------
# SIDKONFIGURATION OCH DESIGN
# --------------------------

# S√§tt sidinst√§llningar f√∂r Streamlit-appen
st.set_page_config(
    page_title="üìñ Bibeln RAG-Chatbot", 
    layout="wide"                       
)

# --------------------------
# SKAPA EGNA PROMPT-TEMPLATE
# --------------------------

# Anv√§nd en PromptTemplate f√∂r att definiera hur fr√•gan och kontexten ska formateras.
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Du √§r en v√§nlig och hj√§lpsam bibelguide som svarar p√• fr√•gor genom att anv√§nda givna bibelavsnitt.
Om du inte hittar relevant information ska du be anv√§ndaren om att skriva om fr√•gan.

Kontekst:
{context}

Fr√•ga:
{question}

Svar:
"""
)

# --------------------------
# FUNKTION F√ñR ATT LADDAR FAISS-INDEX
# --------------------------

@st.cache_resource
def load_retriever(index_path: str):
    """
    Ladda FAISS-index och skapa en retriever-objekt.

    Parametrar:
    - index_path: S√∂kv√§g till mappen d√§r FAISS-index har sparats.

    Returnerar:
    - En retriever som kan anv√§ndas f√∂r likhetss√∂kning.
    """
    # Skapa en embeddings-instans (OpenAI-embeddings h√§mtar API-nyckel fr√•n milj√∂)
    embeddings = OpenAIEmbeddings()

    # Ladda indexet fr√•n disk
    store = FAISS.load_local(
        index_path,
        embeddings,
        allow_dangerous_deserialization=True
    )

    # Returnera retriever med fast antal k√§llor (k=3)
    return store.as_retriever(search_kwargs={"k": 3})

# --------------------------
# L√ÑS IN FAISS-INDEXET
# --------------------------

# Visa en spinner medan kunskapsbasen laddas (FAISS-indexet kan ta en stund)
with st.spinner("Laddar kunskapsbas..."):
    retriever = load_retriever("data/faiss_index")

# --------------------------
# INITIERA LLM OCH QA-KEDJA
# --------------------------

# ChatOpenAI: Wrapper f√∂r OpenAI:s chat-modell (gpt-3.5-turbo)
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",  
    temperature=0                
)

# Skapa RetrievalQA-kedjan med ‚Äústuff‚Äù-metoden och egen prompt
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",               # Enkel kedja som samlar in hela kontexten
    retriever=retriever,
    return_source_documents=False,     # False, f√∂r att inte vill visa k√§llor i svaret
    chain_type_kwargs={"prompt": prompt}  
)

# --------------------------
# KONVERSATIONS-SESSIONSTATE
# --------------------------

# Anv√§nd Streamlit session_state f√∂r att spara tidigare meddelanden 
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Hej! Jag √§r din Bibel-Chatbot. Fr√•ga g√§rna om n√•got bibelst√§lle eller tema, s√• hj√§lper jag dig!"}
    ]

# --------------------------
# RENDERA MESSAGES SOM CHATTBUBBLOR
# --------------------------

# Loopa igenom alla lagrade meddelanden och visa dem med st.chat_message
for msg in st.session_state.messages:
    if msg["role"] == "user":
        
        st.chat_message("user").write(msg["content"])
    else:
        # Visa chatbotens svar som en assistent-bubble
        st.chat_message("assistant").write(msg["content"])

# --------------------------
# INPUTF√ÑLT F√ñR ANV√ÑNDARENS FR√ÖGA
# --------------------------

# st.chat_input visar ett textf√§lt med chattliknande stil
if user_input := st.chat_input("Skriv din fr√•ga h√§r..."):
    
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    # Anropa QA-kedjan f√∂r att f√• svaret
    answer = qa_chain.run(user_input)

    # Hantera tomma eller irrelevanta svar
    if not answer.strip():
        # Ge en standardfeedback om inget svar hittas
        answer = "Jag f√∂rst√•r inte riktigt. Kan du formulera fr√•gan p√• ett annat s√§tt?"
        st.session_state.messages.append({"role": "assistant", "content": answer})
        st.chat_message("assistant").write(answer)
    else:
        # Spara och visa det genererade svaret
        st.session_state.messages.append({"role": "assistant", "content": answer})
        st.chat_message("assistant").write(answer)

# --------------------------
# FOOTER
# --------------------------

# Visa en enkel footer med info om appens verktyg

st.markdown("---")
st.caption("""
üìñ *Svenska Bibel-Chatbot v1.0* | 
Datak√§lla: Svenska Bibels√§llskapet | 
Byggd med Python & LangChain
""")
